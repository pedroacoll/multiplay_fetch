{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/a.a.gonzalez.paje/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import operator\n",
    "import json\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import pyLDAvis.gensim\n",
    "import IPython.display\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/a.a.gonzalez.paje/Box Sync/Alberto/rapid_intel/3.0/code'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/Users/a.a.gonzalez.paje/Box Sync/Alberto/rapid_intel/3.0/code')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listener_results_royalcaribbean.json\n"
     ]
    }
   ],
   "source": [
    "# Load listener\n",
    "term_to_search = 'royalcaribbean'\n",
    "output_file_name = 'listener_results_' + term_to_search + '.json'\n",
    "print output_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66, 36)\n"
     ]
    }
   ],
   "source": [
    "# Load json with tweets\n",
    "input_file = output_file_name\n",
    "df = pd.read_json(input_file, orient = 'records', lines = True)\n",
    "print df.shape\n",
    "#print df.head(2)\n",
    "#print df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning\n",
    "def clean_tweet(tweet):\n",
    "     '''\n",
    "     Utility function to clean the text in a tweet by removing\n",
    "     links and special characters using regex.\n",
    "     '''\n",
    "     return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0              Last cruise sunset enchantmentoftheseas\n",
      "1             LOVED cococayisland enchantmentoftheseas\n",
      "2    Cruise date at Chops Steakhouse on enchantment...\n",
      "3    Vacation is over but we had a blast with frien...\n",
      "4    RT SundayFunday tip run towards the pool run a...\n",
      "Name: text_clean, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Apply the cleaning function above to the column in a dataframe with tweet texts and print head (first 5 rows)\n",
    "df['text_clean'] = np.array([clean_tweet(tweet) for tweet in df['text']])\n",
    "print df['text_clean'].head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_it = stopwords.words('english') + ['rt','RT','via','️','❗','…',\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "\n",
    "def clean_tweet_ngrams(tweet,stemming=False):\n",
    "     '''\n",
    "     Utility function to clean the text in a tweet by trasnforming to lower case,\n",
    "     stemming and removing stopwords\n",
    "     '''\n",
    "     tweet = tweet.lower()\n",
    "     tweet = tweet.split()\n",
    "     if stemming==False:\n",
    "         tweet = [word for word in tweet if not word in stop_it]\n",
    "     else:\n",
    "         ps = PorterStemmer()\n",
    "         tweet = [ps.stem(word) for word in tweet if not word in set(stopwords.words('english'))]\n",
    "     tweet = ' '.join(tweet)\n",
    "     return tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0              last cruise sunset enchantmentoftheseas\n",
      "1             loved cococayisland enchantmentoftheseas\n",
      "2    cruise date chops steakhouse enchantmentofthes...\n",
      "3    vacation blast friends old new safe travels fr...\n",
      "4    sundayfunday tip run towards pool run away monday\n",
      "Name: text_clean_ngrams, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:11: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "#Apply the cleaning function above to the dataframe column with text to clean and print head (first 5 rows)\n",
    "# can be applied to text preprocessed with clean_tweet function for further pre-processing\n",
    "df['text_clean_ngrams'] = np.array([ clean_tweet_ngrams(tweet) for tweet in df['text_clean'] ])    \n",
    "print df['text_clean_ngrams'].head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTransforming to lowercase is optional and can be controlled via the 'lowercase' attribute: set 'lowercase' to 'True' \\nto apply transformation to lower case\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Transforming to lowercase is optional and can be controlled via the 'lowercase' attribute: set 'lowercase' to 'True' \n",
    "to apply transformation to lower case\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via','️','❗','…']\n",
    "\n",
    "def counting_terms(term_type='terms_stop', fname=input_file,nwords =5):\n",
    "    with open(fname, 'r') as f:\n",
    "        count_all=Counter()\n",
    "        for line in f:\n",
    "            tweet=json.loads(line)\n",
    "            # Create a list with  terms\n",
    "            if term_type == 'terms_all':\n",
    "                terms_all = [term for term in preprocess(tweet['text'],True)]            \n",
    "                # Update the counter\n",
    "                count_all.update(terms_all)\n",
    "            elif term_type == 'terms_stop':\n",
    "                terms_stop = [term for term in preprocess(tweet['text'],True) if term not in stop]               \n",
    "                # Update the counter\n",
    "                count_all.update(terms_stop)\n",
    "            elif term_type == 'terms_hash':\n",
    "                terms_hash = [term for term in preprocess(tweet['text'],True) \n",
    "                  if term.startswith('#')]               \n",
    "                # Update the counter\n",
    "                count_all.update(terms_hash)\n",
    "            else:\n",
    "                terms_only = [term for term in preprocess(tweet['text'],True) \n",
    "                  if term not in stop and\n",
    "                  not term.startswith(('#', '@'))] \n",
    "                count_all.update(terms_only)\n",
    "        # Print the first 5 most frequent words\n",
    "        print(count_all.most_common(nwords))\n",
    "        return count_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'.', 46), (u'@royalcaribbean', 43), (u'\\u2026', 38), (u':', 35), (u',', 29), (u'the', 27), (u'for', 20), (u'rt', 18), (u'you', 18), (u'cruise', 15)]\n"
     ]
    }
   ],
   "source": [
    "# All terms\n",
    "term_freq=counting_terms(term_type='terms_all', fname=input_file, nwords = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'@royalcaribbean', 43), (u'\\u2026', 38), (u'cruise', 15), (u'babysitting', 12), (u'\\u2019', 11), (u'in-room', 11), (u'sea', 10), (u'royal', 10), (u'caribbean', 10), (u'sitter', 9)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:15: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# Excluding stopwords\n",
    "term_freq_stop=counting_terms(term_type='terms_stop', fname=input_file,nwords=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'#royalcaribbean', 7), (u'#navigatoroftheseas', 4), (u'#enchantmentoftheseas', 3), (u'#kauai', 2), (u'#sundayfunday', 2), (u'#radianceoftheseas', 2), (u'#cruise', 2), (u'#honduras', 1), (u'#caribbean', 1), (u'#cococayisland', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Only hashtags\n",
    "term_freq_hash=counting_terms(term_type='terms_hash', fname=input_file,nwords=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:25: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'\\u2026', 38), (u'cruise', 15), (u'babysitting', 12), (u'\\u2019', 11), (u'in-room', 11), (u'sea', 10), (u'royal', 10), (u'caribbean', 10), (u'sitter', 9), (u'program', 8)]\n"
     ]
    }
   ],
   "source": [
    "# Terms only\n",
    "term_freq_only=counting_terms(term_type='terms_only', fname=input_file,nwords=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = df['text_clean_ngrams'].apply(nltk.word_tokenize) \n",
    "# Flatening nested list\n",
    "flat_tokens = [term for sublist in tokens for term in sublist]\n",
    "\n",
    "bgs = nltk.bigrams(flat_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(bgs)\n",
    "#for k,v in fdist.items():\n",
    "    #print(k,v)\n",
    "\n",
    "fdist_10 = fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((u'room', u'babysitting'), 12), ((u'royal', u'caribbean'), 10), ((u'sitter', u'sea'), 9), ((u'terminates', u'room'), 8), ((u'popular', u'cruise'), 8), ((u'caribbean', u'discontinues'), 8), ((u'sea', u'program'), 8), ((u'line', u'terminates'), 8), ((u'babysitting', u'service'), 8), ((u'cruise', u'line'), 8)]\n"
     ]
    }
   ],
   "source": [
    "print fdist_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Source_Name</th>\n",
       "      <th>Target_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>(room, babysitting)</td>\n",
       "      <td>12</td>\n",
       "      <td>room</td>\n",
       "      <td>babysitting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>(royal, caribbean)</td>\n",
       "      <td>10</td>\n",
       "      <td>royal</td>\n",
       "      <td>caribbean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>(sitter, sea)</td>\n",
       "      <td>9</td>\n",
       "      <td>sitter</td>\n",
       "      <td>sea</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  bigram  Weight Source_Name  Target_Name\n",
       "303  (room, babysitting)      12        room  babysitting\n",
       "104   (royal, caribbean)      10       royal    caribbean\n",
       "240        (sitter, sea)       9      sitter          sea"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['bigram', 'Weight']\n",
    "\n",
    "df_bigrams = pd.DataFrame([tuple_item for tuple_item in fdist.items()], columns =labels)\n",
    "\n",
    "df_bigrams[['Source_Name','Target_Name']]=pd.DataFrame([tuple_item for tuple_item in df_bigrams.bigram])\n",
    "\n",
    "#sort in descending order\n",
    "df_bigrams.sort_values(by='Weight',ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                bigram  Weight Source_Name Target_Name\n",
      "0     (known, diverse)       1       known     diverse\n",
      "1         (next, week)       1        next        week\n",
      "2  (quantum, brochure)       1     quantum    brochure\n"
     ]
    }
   ],
   "source": [
    "df_bigrams_sub=df_bigrams[df_bigrams['Weight']>0] # Change your weighting filter as needed\n",
    "print df_bigrams_sub.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping format to create output tables\n",
    "#Select multi-level index columns\n",
    "idx =['bigram','Weight']\n",
    "\n",
    "# Then pivot the dataset based on this multi-level index \n",
    "multi_indexed_df = df_bigrams_sub.set_index(idx)\n",
    "multi_indexed_df.head(2)\n",
    "multi_indexed_df.columns.name = 'word1_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bigram               Weight  word1_2    \n",
       "(known, diverse)     1       Source_Name      known\n",
       "                             Target_Name    diverse\n",
       "(next, week)         1       Source_Name       next\n",
       "                             Target_Name       week\n",
       "(quantum, brochure)  1       Source_Name    quantum\n",
       "Name: Label, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stack the columns to achieve the baseline long format for the data\n",
    "stacked_df = multi_indexed_df.stack(dropna=False)\n",
    "stacked_df.name = 'Label'\n",
    "stacked_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             bigram  Weight      word1_2    Label\n",
      "0  (known, diverse)       1  Source_Name    known\n",
      "1  (known, diverse)       1  Target_Name  diverse\n",
      "2      (next, week)       1  Source_Name     next\n"
     ]
    }
   ],
   "source": [
    "# Reset index\n",
    "stacked_df=stacked_df.reset_index()\n",
    "print stacked_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('shape stacked_df:', (778, 4))\n",
      "[u'known' u'diverse']\n",
      "length unique labels: 308\n"
     ]
    }
   ],
   "source": [
    "# Nodes Table\n",
    "# Generate list with unique bigrams\n",
    "print('shape stacked_df:',stacked_df.shape)\n",
    "Label_unique=stacked_df.Label.unique()\n",
    "print Label_unique[0:2]\n",
    "print 'length unique labels:',len(Label_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id    Label\n",
      "0   1    known\n",
      "1   2  diverse\n",
      "2   3     next\n"
     ]
    }
   ],
   "source": [
    "gephi1 = []\n",
    "i=0\n",
    "for item in Label_unique: \n",
    "    i=i+1\n",
    "    gephi1.append({'Label': item, 'Id': i})\n",
    "\n",
    "gephi1=pd.DataFrame(gephi1)\n",
    "print gephi1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict={}\n",
    "i = 0\n",
    "for item in Label_unique:    \n",
    "    i = i+1\n",
    "    mydict[item] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams_nodes_table_royalcaribbean.csv\n"
     ]
    }
   ],
   "source": [
    "output_file_name_nodes = 'bigrams_nodes_table_' + term_to_search + '.csv'\n",
    "print output_file_name_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id    Label\n",
      "0   1    known\n",
      "1   2  diverse\n",
      "2   3     next\n"
     ]
    }
   ],
   "source": [
    "gephi1[['Id','Label']].to_csv(output_file_name_nodes,index=False)\n",
    "print(gephi1.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             bigram  Weight Source_Name Target_Name  Source  Target\n",
      "0  (known, diverse)       1       known     diverse       1       2\n",
      "1      (next, week)       1        next        week       3       4\n"
     ]
    }
   ],
   "source": [
    "# Relations Table\n",
    "df_bigrams_sub['Source']=np.array([mydict[item] for item in df_bigrams_sub['Source_Name']])\n",
    "df_bigrams_sub['Target']=np.array([mydict[item] for item in df_bigrams_sub['Target_Name']])\n",
    "print df_bigrams_sub.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams_relations_table_royalcaribbean.csv\n"
     ]
    }
   ],
   "source": [
    "output_file_name_relations = 'bigrams_relations_table_' + term_to_search + '.csv'\n",
    "print output_file_name_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case, the relations table is generated using nodes lables, not nodes ids.\n",
    "df_bigrams_sub[['Source','Target','Weight']].to_csv(output_file_name_relations,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analize_sentiment(tweet):\n",
    "    '''\n",
    "    Utility function to classify the polarity of a tweet\n",
    "    using textblob    '''\n",
    "    analysis = TextBlob(clean_tweet(tweet))\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 1\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of positive tweets: 45%\n",
      "Percentage of neutral tweets: 42%\n",
      "Percentage of negative tweets: 12%\n"
     ]
    }
   ],
   "source": [
    "df['SA'] = np.array([analize_sentiment(tweet) for tweet in df['text_clean']])\n",
    "\n",
    "pos_tweets = [tweet for index, tweet in enumerate(df['text']) if df['SA'][index] > 0]\n",
    "print \"Percentage of positive tweets: {}%\".format(len(pos_tweets)*100/len(df['text']))\n",
    "\n",
    "neu_tweets = [tweet for index, tweet in enumerate(df['text']) if df['SA'][index] == 0]\n",
    "print \"Percentage of neutral tweets: {}%\".format(len(neu_tweets)*100/len(df['text']))\n",
    "\n",
    "neg_tweets = [tweet for index, tweet in enumerate(df['text']) if df['SA'][index] < 0]\n",
    "print \"Percentage of negative tweets: {}%\".format(len(neg_tweets)*100/len(df['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tables with positive, neutral and negative results\n",
    "neg_tweets_df = df[df['SA']<0]\n",
    "neu_tweets_df = df[df['SA']==0]\n",
    "pos_tweets_df = df[df['SA']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9     RT Kauai you didn t disappoint in dramatic sce...\n",
      "13        No more in room babysitting on royalcaribbean\n",
      "16    RT Kauai you didn t disappoint in dramatic sce...\n",
      "27    very poor experience on Quantum not at all wha...\n",
      "50    Booked my Nov cruise back in Feb Price of balc...\n",
      "51    It s very disappointing when you have to cance...\n",
      "61    Random pictures from the cruise ohshipgirlstri...\n",
      "62    RT Big boat small alley big theatre small scot...\n",
      "Name: text_clean, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Print first negative texts\n",
    "print neg_tweets_df['text_clean'].head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(308 unique tokens: [u'roger', u'approx', u'since', u'respectthewate', u'help']...)\n"
     ]
    }
   ],
   "source": [
    "# Create the dictionary where each term is assigned an index\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "print dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix = [dictionary.doc2bow(tweet) for tweet in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50) # Change the number of topics as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/pyLDAvis/_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n"
     ]
    }
   ],
   "source": [
    "lda_vis = pyLDAvis.gensim.prepare(ldamodel, doc_term_matrix, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el39401120750224167503356694\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el39401120750224167503356694_data = {\"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 1, 2], \"token.table\": {\"Topic\": [2, 2, 3, 3, 2, 1, 1, 2, 1, 3, 3, 2, 3, 1, 2, 1, 3, 3, 2, 3, 1, 3, 2, 2, 1, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 1, 3, 2, 2, 2, 1, 3, 3, 2, 3, 3, 3, 2, 1, 2, 2, 3, 2, 3, 1, 2, 2, 3, 2, 3, 3, 1, 2, 2, 1, 2, 1, 1, 2, 3, 1, 2, 2, 1, 1, 2, 3, 2, 3, 2, 3, 3, 1, 3, 1, 1, 2, 1, 2, 1, 2, 3, 1, 1, 1, 3, 1, 3, 2, 3, 1, 3, 3, 3, 2, 1, 2, 2, 3, 1, 2, 3, 2, 3, 1, 1, 3, 1, 1, 1, 2, 1, 2, 3, 1, 1, 3, 1, 1, 3, 2, 2, 3, 1, 3, 2, 1, 2], \"Freq\": [0.797507107257843, 0.5451619625091553, 0.5451619625091553, 0.8270715475082397, 0.7975071668624878, 0.8955721259117126, 1.0054267644882202, 0.7975070476531982, 0.7143954634666443, 0.2381318211555481, 0.8270722031593323, 0.3339427709579468, 0.6678855419158936, 0.6068687438964844, 0.3034343719482422, 0.986764669418335, 0.8270715475082397, 0.843917191028595, 0.797507107257843, 0.8270715475082397, 0.8955721855163574, 0.8270722031593323, 0.7975070476531982, 0.7975071668624878, 0.5996803045272827, 0.25700584053993225, 0.17133723199367523, 0.8439171314239502, 0.5451340675354004, 0.5451340675354004, 0.5321524143218994, 0.5588133931159973, 0.7975071668624878, 0.797507107257843, 0.797507107257843, 0.5588126182556152, 0.9882971048355103, 0.5588126182556152, 0.797507107257843, 0.5321524143218994, 0.7985897064208984, 0.7784455418586731, 0.38922277092933655, 0.8270722031593323, 0.7975071668624878, 0.5588167309761047, 0.8270715475082397, 0.8270722031593323, 0.5321520566940308, 0.8955721259117126, 0.7975070476531982, 0.7975071668624878, 0.5588180422782898, 0.4142531454563141, 0.4142531454563141, 0.517037570476532, 0.517037570476532, 0.7975070476531982, 0.8270716667175293, 0.7975070476531982, 0.5588126182556152, 0.8439171314239502, 1.0054327249526978, 0.7975071668624878, 0.7985897660255432, 0.5173168182373047, 0.5173168182373047, 0.9882971048355103, 1.0054657459259033, 0.41410088539123535, 0.41410088539123535, 1.0054266452789307, 0.7975071668624878, 0.797507107257843, 0.8955720663070679, 0.7545075416564941, 0.25150251388549805, 0.8439171314239502, 0.5451829433441162, 0.5451829433441162, 0.797507107257843, 0.8270715475082397, 0.8270722031593323, 0.615420401096344, 0.307710200548172, 1.0054267644882202, 0.9882971048355103, 0.5321547985076904, 0.9882971048355103, 0.7975071668624878, 0.5171659588813782, 0.5171659588813782, 0.5588126182556152, 1.0054327249526978, 0.8955721259117126, 0.6609983444213867, 0.2203327715396881, 0.8758666515350342, 0.12512381374835968, 0.6157495975494385, 0.4104997217655182, 0.8955720663070679, 0.843917191028595, 0.8270722031593323, 0.5588126182556152, 0.797507107257843, 0.8154975175857544, 0.13591624796390533, 0.5451705455780029, 0.5451705455780029, 0.9882971048355103, 0.6481775641441345, 0.32408878207206726, 0.5451805591583252, 0.5451805591583252, 0.8889284133911133, 1.0054137706756592, 0.8270722031593323, 0.7492035627365112, 1.0054327249526978, 1.0054267644882202, 0.7975071668624878, 0.9882971048355103, 0.7975070476531982, 0.8270715475082397, 0.8955721259117126, 0.7785788774490356, 0.3892894387245178, 1.0054266452789307, 1.0054267644882202, 0.8270715475082397, 0.5321524143218994, 0.5451874732971191, 0.5451874732971191, 0.8955721259117126, 0.5588184595108032, 0.5321527123451233, 0.7649713158607483, 0.38248565793037415], \"Term\": [\"000\", \"3000\", \"3000\", \"addition\", \"adelanta\", \"amped\", \"away\", \"ayer\", \"babysitting\", \"babysitting\", \"begun\", \"best\", \"best\", \"big\", \"big\", \"caribbean\", \"class\", \"clubs\", \"com\", \"come\", \"coming\", \"countdown\", \"cruceristas\", \"crucero\", \"cruise\", \"cruise\", \"cruise\", \"cruising\", \"date\", \"date\", \"de\", \"deal\", \"del\", \"desembarcan\", \"dice\", \"disappoint\", \"discontinues\", \"dramatic\", \"edici\", \"el\", \"en\", \"enchantmentoftheseas\", \"enchantmentoftheseas\", \"end\", \"eyn\", \"families\", \"features\", \"first\", \"friends\", \"get\", \"getxo\", \"grande\", \"harmony\", \"help\", \"help\", \"hey\", \"hey\", \"hola\", \"hybrid\", \"impresa\", \"kauai\", \"kids\", \"know\", \"l\", \"la\", \"like\", \"like\", \"line\", \"loved\", \"many\", \"many\", \"monday\", \"mundo\", \"n\", \"navigatoroftheseas\", \"new\", \"new\", \"news\", \"next\", \"next\", \"noticia\", \"oasis\", \"oasisoftheseas\", \"one\", \"one\", \"pool\", \"popular\", \"port\", \"program\", \"propiedad\", \"quantum\", \"quantum\", \"radianceoftheseas\", \"rather\", \"ready\", \"room\", \"room\", \"royal\", \"royal\", \"royalcaribbean\", \"royalcaribbean\", \"run\", \"sad\", \"sail\", \"scenery\", \"se\", \"sea\", \"sea\", \"seas\", \"seas\", \"service\", \"ship\", \"ship\", \"ships\", \"ships\", \"sitter\", \"small\", \"sp\", \"stan\", \"stormy\", \"sundayfunday\", \"symphonyoftheseas\", \"terminates\", \"textual\", \"third\", \"thrills\", \"time\", \"time\", \"tip\", \"towards\", \"unique\", \"us\", \"vacation\", \"vacation\", \"way\", \"week\", \"weekend\", \"would\", \"would\"]}, \"mdsDat\": {\"y\": [-0.012099288877934898, -0.08250993335632603, 0.09460922223426092], \"cluster\": [1, 1, 1], \"Freq\": [42.89961624145508, 31.310998916625977, 25.789382934570312], \"topics\": [1, 2, 3], \"x\": [0.1536454625298826, -0.09256637708688234, -0.061079085443000385]}, \"R\": 30, \"lambda.step\": 0.01, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"Term\": [\"caribbean\", \"sitter\", \"line\", \"service\", \"program\", \"terminates\", \"discontinues\", \"popular\", \"royal\", \"room\", \"babysitting\", \"sea\", \"royalcaribbean\", \"cruising\", \"news\", \"kids\", \"clubs\", \"sad\", \"best\", \"en\", \"la\", \"ship\", \"week\", \"harmony\", \"families\", \"deal\", \"radianceoftheseas\", \"disappoint\", \"scenery\", \"dramatic\", \"caribbean\", \"sitter\", \"program\", \"line\", \"discontinues\", \"service\", \"terminates\", \"popular\", \"run\", \"coming\", \"navigatoroftheseas\", \"amped\", \"ready\", \"get\", \"way\", \"thrills\", \"royal\", \"stan\", \"sea\", \"small\", \"away\", \"pool\", \"towards\", \"sundayfunday\", \"monday\", \"tip\", \"stormy\", \"rather\", \"know\", \"loved\", \"room\", \"new\", \"babysitting\", \"cruise\", \"one\", \"big\", \"would\", \"en\", \"la\", \"de\", \"el\", \"us\", \"friends\", \"weekend\", \"port\", \"ship\", \"impresa\", \"desembarcan\", \"n\", \"hola\", \"textual\", \"dice\", \"com\", \"000\", \"ayer\", \"cruceristas\", \"se\", \"noticia\", \"edici\", \"getxo\", \"grande\", \"mundo\", \"del\", \"adelanta\", \"crucero\", \"eyn\", \"symphonyoftheseas\", \"royalcaribbean\", \"l\", \"propiedad\", \"cruise\", \"sea\", \"like\", \"quantum\", \"hey\", \"date\", \"sad\", \"clubs\", \"news\", \"kids\", \"cruising\", \"week\", \"harmony\", \"families\", \"deal\", \"kauai\", \"disappoint\", \"dramatic\", \"radianceoftheseas\", \"scenery\", \"best\", \"oasisoftheseas\", \"countdown\", \"end\", \"begun\", \"first\", \"sp\", \"sail\", \"features\", \"oasis\", \"class\", \"hybrid\", \"addition\", \"come\", \"third\", \"unique\", \"help\", \"many\", \"royalcaribbean\", \"babysitting\", \"room\", \"cruise\", \"royal\", \"time\", \"enchantmentoftheseas\", \"vacation\", \"next\", \"ships\", \"one\", \"seas\", \"3000\"], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.7893999814987183, 0.7818999886512756, 0.7778000235557556, 0.7778000235557556, 0.7778000235557556, 0.7778000235557556, 0.7778000235557556, 0.7778000235557556, 0.7184000015258789, 0.7184000015258789, 0.7184000015258789, 0.7184000015258789, 0.7184000015258789, 0.7184000015258789, 0.7184000015258789, 0.7184000015258789, 0.6980999708175659, 0.6829000115394592, 0.6610000133514404, 0.6205000281333923, 0.6202999949455261, 0.6202999949455261, 0.6202999949455261, 0.6202999949455261, 0.6202999949455261, 0.6202999949455261, 0.6202999949455261, 0.6202999949455261, 0.6202999949455261, 0.6200000047683716, 0.492000013589859, 0.5514000058174133, 0.4603999853134155, 0.33489999175071716, 0.48590001463890076, 0.47690001130104065, 0.3472999930381775, 0.9775000214576721, 0.9775000214576721, 0.9079999923706055, 0.9079999923706055, 0.9078999757766724, 0.907800018787384, 0.907800018787384, 0.9070000052452087, 0.7663999795913696, 0.7529000043869019, 0.7529000043869019, 0.7529000043869019, 0.7529000043869019, 0.7529000043869019, 0.7529000043869019, 0.7529000043869019, 0.7529000043869019, 0.7529000043869019, 0.7529000043869019, 0.7529000043869019, 0.7529000043869019, 0.7529000043869019, 0.7529000043869019, 0.7527999877929688, 0.7527999877929688, 0.7527999877929688, 0.7527999877929688, 0.7527999877929688, 0.7527999877929688, 0.7527999877929688, 0.580299973487854, 0.7527999877929688, 0.7527999877929688, -0.28200000524520874, -0.785099983215332, 0.3334999978542328, 0.32670000195503235, 0.32030001282691956, 0.37310001254081726, 1.1521999835968018, 1.1521999835968018, 1.1521999835968018, 1.1521999835968018, 1.1521999835968018, 1.0764000415802002, 1.0764000415802002, 1.0764000415802002, 1.076300024986267, 1.076300024986267, 1.076300024986267, 1.076300024986267, 1.076300024986267, 1.076300024986267, 0.9225000143051147, 0.9088000059127808, 0.9088000059127808, 0.9088000059127808, 0.9088000059127808, 0.9088000059127808, 0.9088000059127808, 0.9088000059127808, 0.9086999893188477, 0.9086999893188477, 0.9086999893188477, 0.9086999893188477, 0.9086999893188477, 0.9086999893188477, 0.9086999893188477, 0.9086999893188477, 0.7843999862670898, 0.7817999720573425, 0.4237000048160553, 0.13539999723434448, 0.05829999968409538, -0.45159998536109924, -0.8424999713897705, 0.16169999539852142, 0.15809999406337738, 0.49399998784065247, 0.4936000108718872, 0.4934999942779541, -0.07880000025033951, 0.492900013923645, 0.4927000105381012], \"Freq\": [8.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 7.0, 9.0, 8.0, 7.0, 4.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 7.658722400665283, 6.328887462615967, 5.6689043045043945, 5.6689043045043945, 5.6689043045043945, 5.6689043045043945, 5.6689043045043945, 5.6689043045043945, 2.9475133419036865, 2.947510242462158, 2.9475107192993164, 2.947510242462158, 2.947510242462158, 2.947510004043579, 2.947510004043579, 2.947510004043579, 6.89108943939209, 2.2671308517456055, 6.112972736358643, 1.5871100425720215, 1.5868895053863525, 1.5868895053863525, 1.5868895053863525, 1.5868895053863525, 1.5868895053863525, 1.5868895053863525, 1.586785078048706, 1.586785078048706, 1.586785078048706, 1.5862247943878174, 6.368794918060303, 2.960503339767456, 5.709895610809326, 6.999545097351074, 2.2663865089416504, 2.2776923179626465, 1.5873011350631714, 2.084055185317993, 2.084049701690674, 1.4587903022766113, 1.4587823152542114, 1.458662986755371, 1.4585528373718262, 1.4584739208221436, 1.4573723077774048, 2.079078435897827, 0.8335374593734741, 0.8335373401641846, 0.8335373401641846, 0.8335374593734741, 0.8335374593734741, 0.8335373401641846, 0.8335373401641846, 0.8335373401641846, 0.8335374593734741, 0.8335374593734741, 0.8335373401641846, 0.8335373401641846, 0.8335373401641846, 0.8335374593734741, 0.8335203528404236, 0.8335203528404236, 0.8335203528404236, 0.8335203528404236, 0.8335203528404236, 0.8335203528404236, 0.8335203528404236, 2.7252957820892334, 0.8335203528404236, 0.8335203528404236, 2.7569289207458496, 1.0506892204284668, 0.8448114395141602, 0.8394037485122681, 0.8342183232307434, 0.8341073989868164, 1.934474229812622, 1.934474229812622, 1.9344744682312012, 1.9344744682312012, 1.9344744682312012, 1.354103684425354, 1.3540862798690796, 1.3540586233139038, 1.3539618253707886, 1.3539342880249023, 1.3539342880249023, 1.3539342880249023, 1.3539342880249023, 1.3539342880249023, 1.9427165985107422, 0.773686408996582, 0.773686408996582, 0.773686408996582, 0.773686408996582, 0.773686408996582, 0.773686408996582, 0.773686408996582, 0.7736789584159851, 0.7736789584159851, 0.7736789584159851, 0.7736788988113403, 0.7736789584159851, 0.7736789584159851, 0.7736789584159851, 0.7736789584159851, 1.3641303777694702, 1.3610705137252808, 1.9194777011871338, 2.480059862136841, 2.4813718795776367, 1.9164128303527832, 0.8876014351844788, 0.7787312865257263, 0.7760661244392395, 0.7752705216407776, 0.7749655842781067, 0.7748634219169617, 0.7745925188064575, 0.7744298577308655, 0.7742562294006348], \"Total\": [8.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 7.0, 9.0, 8.0, 7.0, 4.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 8.10730266571045, 6.74970006942749, 6.071048736572266, 6.071048736572266, 6.071048736572266, 6.071048736572266, 6.071048736572266, 6.071048736572266, 3.3498141765594482, 3.34981369972229, 3.3498141765594482, 3.349813938140869, 3.349813938140869, 3.349813938140869, 3.349813938140869, 3.349813938140869, 7.99208402633667, 2.669501543045044, 7.357471942901611, 1.9892308712005615, 1.9892051219940186, 1.9892051219940186, 1.9892051219940186, 1.9892051219940186, 1.989205241203308, 1.989205241203308, 1.9891932010650635, 1.9891932010650635, 1.9891932010650635, 1.9891278743743896, 9.077178955078125, 3.9761035442352295, 8.398709297180176, 11.67288589477539, 3.2498111724853516, 3.295605421066284, 2.6144771575927734, 2.5044150352478027, 2.5044147968292236, 1.8791608810424805, 1.8791608810424805, 1.8791608810424805, 1.879162073135376, 1.879159927368164, 1.8791524171829224, 3.085574150085449, 1.2539074420928955, 1.253907322883606, 1.253907322883606, 1.2539074420928955, 1.2539074420928955, 1.253907322883606, 1.253907322883606, 1.253907322883606, 1.2539074420928955, 1.2539074420928955, 1.253907322883606, 1.253907322883606, 1.253907322883606, 1.2539074420928955, 1.2539072036743164, 1.2539072036743164, 1.2539072036743164, 1.2539072036743164, 1.2539072036743164, 1.2539072036743164, 1.2539072036743164, 4.872110366821289, 1.2539072036743164, 1.2539072036743164, 11.67288589477539, 7.357471942901611, 1.9330514669418335, 1.9336152076721191, 1.9340953826904297, 1.8344111442565918, 2.369900703430176, 2.369900703430176, 2.369900941848755, 2.369900941848755, 2.369900941848755, 1.7894898653030396, 1.7894911766052246, 1.7894954681396484, 1.789506196975708, 1.789508581161499, 1.789508581161499, 1.789508581161499, 1.789508581161499, 1.789508581161499, 2.9945249557495117, 1.2090842723846436, 1.2090842723846436, 1.2090842723846436, 1.2090842723846436, 1.2090842723846436, 1.2090842723846436, 1.2090842723846436, 1.20908522605896, 1.20908522605896, 1.20908522605896, 1.2090851068496704, 1.20908522605896, 1.20908522605896, 1.20908522605896, 1.20908522605896, 2.41398286819458, 2.414870500564575, 4.872110366821289, 8.398709297180176, 9.077178955078125, 11.67288589477539, 7.99208402633667, 2.5687828063964844, 2.5692226886749268, 1.8342313766479492, 1.8342466354370117, 1.8342547416687012, 3.2498111724853516, 1.8342883586883545, 1.8343173265457153], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.400099992752075, -3.5908000469207764, -3.7009999752044678, -3.7009999752044678, -3.7009999752044678, -3.7009999752044678, -3.7009999752044678, -3.7009999752044678, -4.355000019073486, -4.355000019073486, -4.355000019073486, -4.355000019073486, -4.355000019073486, -4.355000019073486, -4.355000019073486, -4.355000019073486, -3.50570011138916, -4.617400169372559, -3.625499963760376, -4.973999977111816, -4.9741997718811035, -4.9741997718811035, -4.9741997718811035, -4.9741997718811035, -4.9741997718811035, -4.9741997718811035, -4.9741997718811035, -4.9741997718811035, -4.9741997718811035, -4.974599838256836, -3.5845000743865967, -4.350599765777588, -3.6937999725341797, -3.4900999069213867, -4.617800235748291, -4.612800121307373, -4.973899841308594, -4.3867998123168945, -4.3867998123168945, -4.743500232696533, -4.743500232696533, -4.743500232696533, -4.743599891662598, -4.74370002746582, -4.7444000244140625, -4.389100074768066, -5.303100109100342, -5.303100109100342, -5.303100109100342, -5.303100109100342, -5.303100109100342, -5.303100109100342, -5.303100109100342, -5.303100109100342, -5.303100109100342, -5.303100109100342, -5.303100109100342, -5.303100109100342, -5.303100109100342, -5.303100109100342, -5.303199768066406, -5.303199768066406, -5.303199768066406, -5.303199768066406, -5.303199768066406, -5.303199768066406, -5.303199768066406, -4.118500232696533, -5.303199768066406, -5.303199768066406, -4.106900215148926, -5.071599960327148, -5.289700031280518, -5.29610013961792, -5.302299976348877, -5.302499771118164, -4.267199993133545, -4.267199993133545, -4.267199993133545, -4.267199993133545, -4.267199993133545, -4.623899936676025, -4.623899936676025, -4.624000072479248, -4.624000072479248, -4.624000072479248, -4.624000072479248, -4.624000072479248, -4.624000072479248, -4.624000072479248, -4.263000011444092, -5.183599948883057, -5.183599948883057, -5.183599948883057, -5.183599948883057, -5.183599948883057, -5.183599948883057, -5.183599948883057, -5.183700084686279, -5.183700084686279, -5.183700084686279, -5.183700084686279, -5.183700084686279, -5.183700084686279, -5.183700084686279, -5.183700084686279, -4.616499900817871, -4.618800163269043, -4.275000095367432, -4.018799781799316, -4.018199920654297, -4.276599884033203, -5.046299934387207, -5.17710018157959, -5.180600166320801, -5.181600093841553, -5.182000160217285, -5.18209981918335, -5.182499885559082, -5.182700157165527, -5.1828999519348145]}};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el39401120750224167503356694\", ldavis_el39401120750224167503356694_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el39401120750224167503356694\", ldavis_el39401120750224167503356694_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el39401120750224167503356694\", ldavis_el39401120750224167503356694_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following code will generate an HTML file with the Topic Modelling results\n",
    "pyLDAvis.enable_notebook(local=False)\n",
    "pyLDAvis.save_html(lda_vis,\"data/topic_modeling_twitter.html\") # Change the output name as needed\n",
    "pyLDAvis.display(lda_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66, 6)\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "tf_vectorizer = CountVectorizer(min_df = 10)\n",
    "dtm_tf = tf_vectorizer.fit_transform(df['text_clean_ngrams'])\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print dtm_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66, 6)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(**tf_vectorizer.get_params())\n",
    "dtm_tfidf = tfidf_vectorizer.fit_transform(df['text_clean_ngrams'])\n",
    "print dtm_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
       "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
       "             mean_change_tol=0.001, n_components=5, n_jobs=1,\n",
       "             n_topics=None, perp_tol=0.1, random_state=0,\n",
       "             topic_word_prior=None, total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for TF DTM\n",
    "lda_tf = LatentDirichletAllocation(n_components=5, random_state=0)\n",
    "lda_tf.fit(dtm_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words): \n",
    "    for topic_idx, topic in enumerate(model.components_): \n",
    "        print(\"Topic %d:\" % (topic_idx)) \n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "cruise babysitting sea caribbean royal room\n",
      "Topic 1:\n",
      "babysitting room sea royal caribbean cruise\n",
      "Topic 2:\n",
      "cruise royal caribbean sea room babysitting\n",
      "Topic 3:\n",
      "room babysitting cruise royal sea caribbean\n",
      "Topic 4:\n",
      "sea room babysitting caribbean royal cruise\n"
     ]
    }
   ],
   "source": [
    "no_top_words = 10\n",
    "display_topics(lda_tf, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
